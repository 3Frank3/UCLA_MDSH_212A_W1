---
title: "Biostat 212a Homework 1"
subtitle: "Due Jan 25, 2026 @ 11:59PM"
author: "YOUR NAME and UID"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
editor: 
  markdown: 
    wrap: 72
---

## Filling gaps in lecture notes (10% pts)

Consider the regression model $$
Y = f(X) + \epsilon,
$$ where $\operatorname{E}(\epsilon) = 0$.

### Optimal regression function

Show that the choice $$
f_{\text{opt}}(X) = \operatorname{E}(Y | X)
$$ minimizes the mean squared prediction error $$
\operatorname{E}\{[Y - f(X)]^2\},
$$ where the expectations averages over variations in both $X$ and $Y$.
(Hint: condition on $X$.)

### Bias-variance trade-off

Given an estimate $\hat f$ of $f$, show that the test error at a $x_0$
can be decomposed as $$
\operatorname{E}\{[y_0 - \hat f(x_0)]^2\} = \underbrace{\operatorname{Var}(\hat f(x_0)) + [\operatorname{Bias}(\hat f(x_0))]^2}_{\text{MSE of } \hat f(x_0) \text{ for estimating } f(x_0)} + \underbrace{\operatorname{Var}(\epsilon)}_{\text{irreducible}},
$$ where the expectation averages over the variability in $y_0$ and
$\hat f$.

## ISL Exercise 2.4.3 (10% pts)

*2.4.3. We now revisit the bias-variance decomposition.* (a) Provide a
sketch of typical (squared) bias, variance, training error, test error,
and Bayes (or irreducible) error curves, on a single plot, as we go from
less flexible statistical learning methods towards more flexible
approaches. The x-axis should represent the amount of flexibility in the
method, and the y-axis should represent the values for each curve. There
should be five curves. Make sure to label each one.

**Solution to 2.4.3(a)**

```{r, eval = F}
# (a) Plot the curves
## Bias-Variance Decomposition Curves (Conceptual Sketch)
## 5 curves: squared bias, variance, training error, test error, Bayes error

# 1) x-axis: flexibility grid
flex <- seq(0, 1, length.out = 300)

# 2) Construct typical curve shapes (purely illustrative)
bias2 <- (1 - flex)^2 + 0.02                  # decreasing
var   <- 0.02 + 0.9 * flex^2                  # increasing
bayes <- rep(0.15, length(flex))              # flat (irreducible error)

train_err <- 0.85 * exp(-4.5 * flex) + 0.05   # decreasing, can approach ~0

# test error roughly = bias^2 + variance + irreducible error (plus a tiny wiggle)
test_err <- bias2 + var + bayes

# 3) Plot everything on one figure
ylim_max <- max(bias2, var, train_err, test_err, bayes) * 1.05

plot(
  flex, bias2, type = "l", lwd = 3,
  xlab = "Flexibility (low  \u2192  high)",
  ylab = "Error / Quantity",
  ylim = c(0, ylim_max),
  main = "Bias-Variance Decomposition: Typical Curves"
)

lines(flex, var,       lwd = 3, lty = 2)
lines(flex, train_err, lwd = 3, lty = 3)
lines(flex, test_err,  lwd = 3, lty = 1)
lines(flex, bayes,     lwd = 3, lty = 4)

legend(
  "topright", bty = "n", lwd = 3,
  legend = c("Squared Bias", "Variance", "Training Error", "Test Error", "Bayes (Irreducible) Error"),
  lty = c(1, 2, 3, 1, 4)
)

# 4) Optional: mark the minimum of test error
min_idx <- which.min(test_err)
points(flex[min_idx], test_err[min_idx], pch = 19)
text(flex[min_idx], test_err[min_idx],
     labels = "Min test error", pos = 4, cex = 0.9)

```

**Solution to 2.4.3(b)** 
```{r, eval = F}
cat("
### (b) Explanation of the Five Curves

**Squared Bias**  
Squared bias decreases as model flexibility increases because more flexible models
can better approximate the true underlying function. Simple models impose strong
structural assumptions, leading to systematic underfitting and high bias.

**Variance**  
Variance increases with flexibility because flexible models are more sensitive to
random fluctuations in the training data. Small changes in the sample can lead to
large changes in the fitted model.

**Training Error**  
Training error decreases monotonically as flexibility increases because more
flexible models can fit the training data more closely, including noise in the data.

**Test Error**  
Test error is U-shaped as a function of flexibility. Initially, increasing
flexibility reduces bias faster than it increases variance, leading to lower test
error. Beyond an optimal level, variance dominates and test error increases due to
overfitting.

**Bayes (Irreducible) Error**  
The Bayes error remains constant across all levels of flexibility because it
represents variability in the response that cannot be explained by any model,
regardless of complexity.
")
```

```{r, eval = F}
library(tidyverse)
fit <- lm(sales ~ TV, data = )
```

## ISL Exercise 2.4.4 (10% pts)

## ISL Exercise 2.4.10 (30% pts)

Your can read in the `boston` data set directly from url
<https://raw.githubusercontent.com/ucla-biostat-212a/2026winter/master/slides/data/Boston.csv>.
A documentation of the `boston` data set is
[here](https://www.rdocumentation.org/packages/ISLR2/versions/1.3-2/topics/Boston).

```{r, evalue = F}
library(tidyverse)

Boston <- read_csv("https://raw.githubusercontent.com/ucla-biostat-212a/2026winter/master/slides/data/Boston.csv", col_select = -1) %>% 
  print(width = Inf)
```

## ISL Exercise 3.7.3 (20% pts)

## ISL Exercise 3.7.15 (20% pts)

## Bonus question (Extra credits)

For multiple linear regression, show that $R^2$ is equal to the
correlation between the response vector
$\mathbf{y} = (y_1, \ldots, y_n)^T$ and the fitted values
$\hat{\mathbf{y}} = (\hat y_1, \ldots, \hat y_n)^T$. That is $$
R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = [\operatorname{Cor}(\mathbf{y}, \hat{\mathbf{y}})]^2.
$$
