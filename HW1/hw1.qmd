---
title: "Biostat 212a Homework 1"
subtitle: "Due Jan 25, 2026 @ 11:59PM"
author: "YOUR NAME and UID"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
editor: 
  markdown: 
    wrap: 72
---

## Filling gaps in lecture notes (10% pts)

Consider the regression model $$
Y = f(X) + \epsilon,
$$ where $\operatorname{E}(\epsilon) = 0$.

### Optimal regression function

Show that the choice $$
f_{\text{opt}}(X) = \operatorname{E}(Y | X)
$$ minimizes the mean squared prediction error $$
\operatorname{E}\{[Y - f(X)]^2\},
$$ where the expectations averages over variations in both $X$ and $Y$.
(Hint: condition on $X$.)

### Bias-variance trade-off

Given an estimate $\hat f$ of $f$, show that the test error at a $x_0$
can be decomposed as $$
\operatorname{E}\{[y_0 - \hat f(x_0)]^2\} = \underbrace{\operatorname{Var}(\hat f(x_0)) + [\operatorname{Bias}(\hat f(x_0))]^2}_{\text{MSE of } \hat f(x_0) \text{ for estimating } f(x_0)} + \underbrace{\operatorname{Var}(\epsilon)}_{\text{irreducible}},
$$ where the expectation averages over the variability in $y_0$ and
$\hat f$.

## ISL Exercise 2.4.3 (10% pts)

*2.4.3. We now revisit the bias-variance decomposition.* (a) Provide a
sketch of typical (squared) bias, variance, training error, test error,
and Bayes (or irreducible) error curves, on a single plot, as we go from
less flexible statistical learning methods towards more flexible
approaches. The x-axis should represent the amount of flexibility in the
method, and the y-axis should represent the values for each curve. There
should be five curves. Make sure to label each one.

**Solution to 2.4.3(a)**

```{r, eval = T}
# (a) Plot the curves
## Bias-Variance Decomposition Curves (Conceptual Sketch)
## 5 curves: squared bias, variance, training error, test error, Bayes error

# 1) x-axis: flexibility grid
flex <- seq(0, 1, length.out = 300)

# 2) Construct typical curve shapes (purely illustrative)
bias2 <- (1 - flex)^2 + 0.02                  # decreasing
var   <- 0.02 + 0.9 * flex^2                  # increasing
bayes <- rep(0.15, length(flex))              # flat (irreducible error)

train_err <- 0.85 * exp(-4.5 * flex) + 0.05   # decreasing, can approach ~0

# test error roughly = bias^2 + variance + irreducible error (plus a tiny wiggle)
test_err <- bias2 + var + bayes

# 3) Plot everything on one figure
ylim_max <- max(bias2, var, train_err, test_err, bayes) * 1.05

plot(
  flex, bias2, type = "l", lwd = 3,
  xlab = "Flexibility (low  \u2192  high)",
  ylab = "Error / Quantity",
  ylim = c(0, ylim_max),
  main = "Bias-Variance Decomposition: Typical Curves"
)

lines(flex, var,       lwd = 3, lty = 2)
lines(flex, train_err, lwd = 3, lty = 3)
lines(flex, test_err,  lwd = 3, lty = 1)
lines(flex, bayes,     lwd = 3, lty = 4)

legend(
  "topright", bty = "n", lwd = 3,
  legend = c("Squared Bias", "Variance", "Training Error", "Test Error", "Bayes (Irreducible) Error"),
  lty = c(1, 2, 3, 1, 4)
)

# 4) Optional: mark the minimum of test error
min_idx <- which.min(test_err)
points(flex[min_idx], test_err[min_idx], pch = 19)
text(flex[min_idx], test_err[min_idx],
     labels = "Min test error", pos = 4, cex = 0.9)

```

**Solution to 2.4.3(b)** *(b) Explain why each curve has the shape in
(a)*

#### Squared bias (decreases with flexibility)

Bias measures **systematic error** from using an overly restrictive
model class.\
When flexibility is **low**, the model cannot represent the true signal
(f(X)) well (underfitting), so the bias is **large**.\
As flexibility increases, the model class can approximate (f(X)) more
accurately, so **squared bias decreases**.

#### Variance (increases with flexibility)

Variance measures how much the fitted function would change if we
trained the model on a different sample from the same population.

When model flexibility is **low**, the fitted model is highly
constrained and tends to be stable across different training samples,
resulting in **low variance**.

As model flexibility increases, the method becomes more sensitive to the
particular training data, including random noise. Consequently,
predictions can vary substantially across different samples, leading to
**higher variance**.

#### Training error (monotonically decreases)

Training error is computed on the **same data used to fit the model**.\
More flexibility gives the method more freedom to fit the training
points, so training error **cannot increase** as flexibility increases
and typically **decreases**, often approaching (0) for very flexible
methods.

#### Test error (U-shaped)

Test error measures how well the model generalizes to new, unseen data.\
It is governed by the biasâ€“variance tradeoff:

$$
\mathbb{E}\left[ \{Y - \hat f(X)\}^2 \right]
=
\text{Bias}^2
+
\text{Variance}
+
\text{Irreducible Error}.
$$

When model flexibility is **low**, the model suffers from high bias due
to underfitting, leading to a **large test error**.

As flexibility increases, the reduction in bias dominates the increase
in variance, causing the **test error to decrease**.

When flexibility becomes **very high**, variance increases substantially
due to overfitting the training data, resulting in an **increase in test
error**.

This produces the characteristic **U-shaped curve** for test error as a
function of model flexibility.

#### Bayes (irreducible) error (flat)

The irreducible error is due to randomness in (Y\mid X) (noise,
unmeasured factors, inherent variability).\
No matter how flexible the method is, this component cannot be
eliminated, so it is **constant** across flexibility and appears as a
**horizontal line**.

```{r, eval = F}
library(tidyverse)
fit <- lm(sales ~ TV, data = )
```

## ISL Exercise 2.4.4 (10% pts)

## ISL Exercise 2.4.10 (30% pts)

Your can read in the `boston` data set directly from url
<https://raw.githubusercontent.com/ucla-biostat-212a/2026winter/master/slides/data/Boston.csv>.
A documentation of the `boston` data set is
[here](https://www.rdocumentation.org/packages/ISLR2/versions/1.3-2/topics/Boston).

```{r, evalue = F}
library(tidyverse)

Boston <- read_csv("https://raw.githubusercontent.com/ucla-biostat-212a/2026winter/master/slides/data/Boston.csv", col_select = -1) %>% 
  print(width = Inf)
```

## ISL Exercise 3.7.3 (20% pts)

## ISL Exercise 3.7.15 (20% pts)

## Bonus question (Extra credits)

For multiple linear regression, show that $R^2$ is equal to the
correlation between the response vector
$\mathbf{y} = (y_1, \ldots, y_n)^T$ and the fitted values
$\hat{\mathbf{y}} = (\hat y_1, \ldots, \hat y_n)^T$. That is $$
R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = [\operatorname{Cor}(\mathbf{y}, \hat{\mathbf{y}})]^2.
$$
